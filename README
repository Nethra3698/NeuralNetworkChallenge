Using the rule of thumb I estimated that the first layer should have about two - three times the number of input features, which is what I ended up using. There were 50 input features and hence I used twice the number for the first layer and half of this for the second layer.I used relu as the activation function for both these layers. I tried adding a third layer with half the number of neurons(25) and also tried it with leaky Relu activation but the accuracy I obtained was the same as with the 2 layers and hence realized that the third layer did not make much of a change. I also tried to change the activation for the first hidden layer to tanh but this just gave me the same accuracy. 
Overall, all the changes I made in activation, number of nearuons and number of layers all gave me the same accuarcy of ~ 0.726. 
This is below the target model performance of 75%. Other than the methods I previously mentioned. I dropped columns like name and ein that are not required for what we want to achieve. 
But ultimately this is the model that I would choose as the random forest classifier method gives an accuracy of ~0.71